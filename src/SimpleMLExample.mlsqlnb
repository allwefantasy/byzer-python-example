[{"kind":1,"language":"markdown","value":"## Buildin ML Algorithm","outputs":[]},{"kind":2,"language":"mlsql","value":"include project.`./src/common/mock_data.mlsql`;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"features\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\"containsNull\": true\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"label\",\n\t\t\t\t\"type\": \"double\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t4.4,\n\t\t\t\t2.9,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t4.7,\n\t\t\t\t3.2,\n\t\t\t\t1.3,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": [\n\t\t\t\t5.1,\n\t\t\t\t3.5,\n\t\t\t\t1.4,\n\t\t\t\t0.2\n\t\t\t],\n\t\t\t\"label\": 0\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"-- create mock validate/test dataset.\nselect vec_dense(features) as features, label as label from mock_data as mock_data_1;\nselect * from mock_data_1 as mock_data_validate;\nselect * from mock_data_1 as mock_data_test;\n-- !desc mock_data_1;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"features\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"udt\",\n\t\t\t\t\t\"class\": \"org.apache.spark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"pyClass\": \"pyspark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"sqlType\": {\n\t\t\t\t\t\t\"type\": \"struct\",\n\t\t\t\t\t\t\"fields\": [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"type\",\n\t\t\t\t\t\t\t\t\"type\": \"byte\",\n\t\t\t\t\t\t\t\t\"nullable\": false,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"size\",\n\t\t\t\t\t\t\t\t\"type\": \"integer\",\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"indices\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"integer\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"values\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"label\",\n\t\t\t\t\"type\": \"double\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t4.4,\n\t\t\t\t\t2.9,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t4.7,\n\t\t\t\t\t3.2,\n\t\t\t\t\t1.3,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"!show et;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"algType\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"sparkCompatibility\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"doc\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"docType\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"name\": \"ALSInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n<a href=\\\"https://en.wikipedia.org/wiki/Matrix_completion\\\">Alternating Least Squares (ALS)</a>\\n\\nThe alternating least squares (ALS) algorithm factorizes a given matrix R into two factors U and\\nV such that R≈UTV. The unknown row dimension is given as a parameter to the algorithm and is\\ncalled latent factors. Since matrix factorization can be used in the context of recommendation,\\nthe matrices U and V can be called user and item matrix, respectively.\\n\\n Use \\\"load modelParams.`ALSInPlace` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Assert\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"AutoIncrementKeyExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"AutoML\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n AutoML is an extension for finding the best models among GBT, LinearRegression,LogisticRegression,\\n NaiveBayes and RandomForest classifiers.\\n\\n It only supports binary labels and sorted by custmoized performance key.\\n\\n Use \\\"load modelParams.`AutoML` as output;\\\"\\n\\n to check the available parameters;\\n\\n Use \\\"load modelExample.`AutoML` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"AutoML\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Binning\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"CacheExt\",\n\t\t\t\"algType\": \"feature engineer\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\nSQLCacheExt is used to cache/uncache table.\\n\\n```sql\\nrun table as CacheExt.`` where execute=\\\"cache\\\" and isEager=\\\"true\\\";\\n```\\n\\nIf you execute the upper command, then table will be cached immediately, othersise only the second time\\nto use the table you will fetch the table from cache.\\n\\nTo release the table , do like this:\\n\\n```sql\\nrun table as CacheExt.`` where execute=\\\"uncache\\\";\\n```\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ClassificationEvaluator\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\nCompute f1|weightedPrecision|weightedRecall|accuracy for predicted table.\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ColumnsExt\",\n\t\t\t\"algType\": \"feature engineer\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n\\n\",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"CommunityBasedSimilarityInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ConfusionMatrix\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"CopyFromLocal\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"CorpusExplainInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DTF\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DataMissingValueProcess\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DataSourceExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DataSummary\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DataSummaryT\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DataTranspose\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DeltaCommandWrapper\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DeltaCompactionCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DescriptiveMetrics\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DicOrTableToArray\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Discretizer\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Discretization\\\">Discretization</a>\\nIn applied mathematics, discretization is the process of transferring continuous functions,\\nmodels, variables, and equations into discrete counterparts.\\nThis process is usually carried out as a first step toward making them suitable for numerical\\nevaluation and implementation on digital computers. Dichotomization is the special case of\\ndiscretization in which the number of discrete classes is 2,\\nwhich can approximate a continuous variable as a binary variable\\n(creating a dichotomy for modeling purposes, as in binary classification).\\n\\n Use \\\"load modelParams.`Discretizer` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"DownloadExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ElifCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ElseCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"EmptyTable\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"EmptyTableWithSchema\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"EngineResource\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ExternalPythonAlg\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n\\nRequirements:\\n\\n1. Conda is installed in your cluster.\\n2. The user who runs StreamingPro cluster has the permission to read/write `/tmp/__mlsql__`.\\n\\nSuppose you run StreamingPro/MLSQL with user named `mlsql`.\\nConda should be installed by `mlsql` and `mlsql` have the permission to read/write `/tmp/__mlsql__`.\\n\\nYou can get code example by:\\n\\n```\\nload modelExample.`PythonAlg` as output;\\n```\\n\\nActually, this doc is also can be get by this command.\\n\\nIf you wanna know what params the PythonAlg have, please use the command following:\\n\\n```\\nload modelParam.`PythonAlg` as output;\\n```\\n\\n     \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"FPGrowth\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"FeatureExtractInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"FeishuMessageExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n FeishuMessageExt sends specified text to a feishu webhook.\\n Please follow https://www.feishu.cn/hc/zh-CN/articles/360024984973 to get a webhook\\n Use \\\"load modelExample.`FeishuMessageExt` as output;\\\" to see the codeExample.\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"FiCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"GBTClassifier\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Gradient_boosting\\\">Gradient Boosting</a> is a\\n machine learning technique for regression, classification and other tasks,\\n which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees\\n\\n Use \\\"load modelParams.`GBTClassifier` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`GBTClassifier` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"GBTClassifier\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"GBTRegressor\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Gradient_boosting\\\">Gradient Boosting</a> is a\\n machine learning technique for regression, classification and other tasks,\\n which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees\\n\\n Use \\\"load modelParams.`GBTRegressor` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`GBTRegressor` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"GBTRegressor\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"GBTs\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"HDFSCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"HashTfIdf\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"IfCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"IteratorCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"JDBC\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"JDBCUpdatExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"JsonExpandExt\",\n\t\t\t\"algType\": \"feature engineer\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n JsonExpandExt is used to expand json strings, please\\n see the codeExample to learn its usage.\\n\\n Use \\\"load modelExample.`JsonExpandExt` as output;\\\"\\n to see the codeExample.\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"KMeans\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n<a href=\\\"https://en.wikipedia.org/wiki/K-means_clustering\\\"> k-means clustering </a>\\n\\nk-means clustering is a method of vector quantization, originally from signal processing,\\nthat aims to partition n observations into k clusters in which each observation belongs to\\nthe cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype\\nof the cluster. This results in a partitioning of the data space into Voronoi cells.\\nk-means clustering minimizes within-cluster variances (squared Euclidean distances),\\nbut not regular Euclidean distances, which would be the more difficult Weber problem:\\nthe mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances.\\nFor instance, better Euclidean solutions can be found using k-medians and k-medoids.\\n\\n Use \\\"load modelParams.`KMeans` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"KafkaCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Kill\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LDA\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"http://en.wikipedia.org/wiki/LDA\\\">LDA</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n1. To load data\\n```\\nload libsvm.`D:/soucecode/spark-2.3-src/data/mllib/sample_lda_libsvm_data.txt` as data1;\\n```\\n\\n2. To train LDA Model\\n```\\ntrain data1 as LDA.`/tmp/model` where\\n```\\n\\n-- k: number of topics, or number of clustering centers\\n```\\nk=\\\"3\\\"\\n```\\n\\n-- docConcentration: the hyperparameter (Dirichlet distribution parameter) of article distribution must be >1.0. The larger the value is, the smoother the predicted distribution is\\n```\\nand docConcentration=\\\"3.0\\\"\\n```\\n\\n-- topictemperature: the hyperparameter (Dirichlet distribution parameter) of the theme distribution must be >1.0. The larger the value is, the more smooth the distribution can be inferred\\n```\\nand topicConcentration=\\\"3.0\\\"\\n```\\n\\n-- maxIterations: number of iterations, which need to be fully iterated, at least 20 times or more\\n```\\nand maxIter=\\\"100\\\"\\n```\\n\\n-- setSeed: random seed\\n```\\nand seed=\\\"10\\\"\\n```\\n\\n-- checkpointInterval: interval of checkpoints during iteration calculation\\n```\\nand checkpointInterval=\\\"10\\\"\\n```\\n\\n-- optimizer: optimized calculation method currently supports \\\"em\\\" and \\\"online\\\". Em method takes up more memory, and multiple iterations of memory may not be enough to throw a stack exception\\n```\\nand optimizer=\\\"online\\\";\\n```\\n\\n3. register LDA to UDF\\n```\\nregister LDA.`C:/tmp/model` as lda;\\n```\\n\\n4. use LDA udf\\n```\\nselect label,lda(4) topicsMatrix,lda_doc(features) TopicDistribution,lda_topic(label,4) describeTopics from data as result;\\n```\\n\\n5. save result\\n```\\nsave overwrite result as json.`/tmp/result`;\\n```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LSVM\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LastCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\nWhen you want to get the result from command and used\\n in next command(SQL), you can use !last command.\\n\\nFor example:\\n\\n```\\n\\n!hdfs /tmp;\\n!last named hdfsTmpTable;\\nselect * from hdfsTmpTable;\\n    \\n```\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LastTableName\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LinearRegression\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Linear_Regression\\\">Linear Regression</a> learning algorithm for\\n classification.\\n It usually used for prediction/forecasting/error reduction and variation explain.\\n\\n Use \\\"load modelParams.`LinearRegression` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`LinearRegression` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"LinearRegression\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"LogisticRegression\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Logistic_regression\\\">Logistic Regression</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n  Outputs with more than two values are modeled by multinomial logistic regression and,\\n  if the multiple categories are ordered, by ordinal logistic regression\\n  (for example the proportional odds ordinal logistic mode）\\n\\n Use \\\"load modelParams.`LogisticRegression` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`LogisticRegression` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"LogisticRegression\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"MLSQLEventCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"MLSQLWatcherCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Map\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"MapValues\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ModelCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ModelExplainInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"NaiveBayes\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\\\">Naive Bayes</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n Use \\\"load modelParams.`NaiveBayes` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`NaiveBayes` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"NaiveBayes\\\" as output;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"NormalizeInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Normalization_(statistics)\\\">Normalization</a>\\n\\n In statistics and applications of statistics,\\n normalization can have a range of meanings.\\n In the simplest cases, normalization of ratings means adjusting values measured on different scales to a\\n notionally common scale, often prior to averaging. In more complicated cases, normalization may refer to\\n more sophisticated adjustments where the intention is to bring the entire probability distributions of adjusted\\n values into alignment. In the case of normalization of scores in educational assessment, there may be an\\n intention to align distributions to a normal distribution.\\n A different approach to normalization of probability distributions is quantile normalization, where the\\n quantiles of the different measures are brought into alignment.\\n\\n Use \\\"load modelParams.`NormalizeInPlace` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"NothingET\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Onehot\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PSI\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PageRank\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PatternDistribution\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Pivot\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PluginCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PredictionEva\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers\\\"> PredictionEva </a> is an extension for evaluating the prediction result.\\n\\n It will evaluate the prediction results from f1-score, precision, recall, auc value and prc value.\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PrintCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ProfilerCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PythonAlg\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n\\nRequirements:\\n\\n1. Conda is installed in your cluster.\\n2. The user who runs StreamingPro cluster has the permission to read/write `/tmp/__mlsql__`.\\n\\nSuppose you run StreamingPro/MLSQL with user named `mlsql`.\\nConda should be installed by `mlsql` and `mlsql` have the permission to read/write `/tmp/__mlsql__`.\\n\\nYou can get code example by:\\n\\n```\\nload modelExample.`PythonAlg` as output;\\n```\\n\\nActually, this doc is also can be get by this command.\\n\\nIf you wanna know what params the PythonAlg have, please use the command following:\\n\\n```\\nload modelParam.`PythonAlg` as output;\\n```\\n\\n     \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PythonAlgBP\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PythonCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PythonEnvExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PythonInclude\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n\\n```\\n\\nexample\\n    \\n```\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"PythonParallelExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RandomForest\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"http://en.wikipedia.org/wiki/Random_forest\\\">Random Forest</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n Use \\\"load modelParams.`RandomForest` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`RandomForest` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"RandomForest\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RateSampler\",\n\t\t\t\"algType\": \"feature engineer\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n Splits dataset into train and test, splitting ratio is specified\\n by parameter sampleRate.\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RawSimilarInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"Ray\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \" \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ReduceFeaturesInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RegressionEvaluator\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\nCompute mse|rmse|r2|mae for predicted table.\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RepartitionExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RowMatrix\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"RunScript\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\nWhen you want to get the result from command and used\\n in next command(SQL), you can use !last command.\\n\\nFor example:\\n\\n```\\n\\n!runScript ''' select 1 as a as b; ''' named output;\\n    \\n```\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"SampleDatasetExt\",\n\t\t\t\"algType\": \"feature engineer\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n## SampleDataset\\n\\nSampleDataset is used to create sample data for Machine learning.\\nWith this plugin help, the users can create a table contains id,features,label columns\\nand we can control the table size, the features size and label size.\\n\\n\",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"SaveBinaryAsFile\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ScalaScriptUDF\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n## Script support\\n\\nScript e.g. Python,Scala nested in MLSQL provides more fine-grained control when doing some ETL tasks, as it allows you\\neasily create SQL function with more powerful language which can do complex logical task.\\n\\nCause the tedious of java's grammar, we will not support java script.\\n\\nBefore use ScriptUDF module, you can use\\n\\n```\\nload modelParams.`ScriptUDF` as output;\\n```\\n\\nto check how to configure this module.\\n\\n### Python UDF Script Example\\n\\n```sql\\n-- using set statement to hold your python script\\n-- Notice that the first parameter of function you defined should be self.\\nset echoFun='''\\n\\ndef apply(self,m):\\n    return m\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be processed more conveniently.\\nload script.`echoFun` as scriptTable;\\n\\n-- register `apply` as UDF named `echoFun`\\nregister ScriptUDF.`scriptTable` as echoFun options\\n-- specify which script you choose\\nand lang=\\\"python\\\"\\n-- As we know python is not strongly typed language, so\\n-- we should manually spcify the return type.\\n-- map(string,string) means a map with key is string type,value also is string type.\\n-- array(string) means a array with string type element.\\n-- nested is support e.g. array(array(map(string,array(string))))\\nand dataType=\\\"map(string,string)\\\"\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect echoFun(map('a','b')) as res from dataTable as output;\\n```\\n\\n### Scala UDF Script Example\\n\\n```sql\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be process more convenient.\\nload script.`plusFun` as scriptTable;\\n\\n-- register `apply` as UDF named `plusFun`\\nregister ScriptUDF.`scriptTable` as plusFun\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as res from dataTable as output;\\n```\\n\\n\\n### Python UDAF Example\\n\\n```sql\\nset plusFun='''\\nfrom org.apache.spark.sql.expressions import MutableAggregationBuffer, UserDefinedAggregateFunction\\nfrom org.apache.spark.sql.types import DataTypes,StructType\\nfrom org.apache.spark.sql import Row\\nimport java.lang.Long as l\\nimport java.lang.Integer as i\\n\\nclass SumAggregation:\\n\\n    def inputSchema(self):\\n        return StructType().add(\\\"a\\\", DataTypes.LongType)\\n\\n    def bufferSchema(self):\\n        return StructType().add(\\\"total\\\", DataTypes.LongType)\\n\\n    def dataType(self):\\n        return DataTypes.LongType\\n\\n    def deterministic(self):\\n        return True\\n\\n    def initialize(self,buffer):\\n        return buffer.update(i(0), l(0))\\n\\n    def update(self,buffer, input):\\n        sum = buffer.getLong(i(0))\\n        newitem = input.getLong(i(0))\\n        buffer.update(i(0), l(sum + newitem))\\n\\n    def merge(self,buffer1, buffer2):\\n        buffer1.update(i(0), l(buffer1.getLong(i(0)) + buffer2.getLong(i(0))))\\n\\n    def evaluate(self,buffer):\\n        return buffer.getLong(i(0))\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\nand lang=\\\"python\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n### Scala UDAF Script Example\\n\\n```sql\\nset plusFun='''\\nimport org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\\nimport org.apache.spark.sql.types._\\nimport org.apache.spark.sql.Row\\nclass SumAggregation extends UserDefinedAggregateFunction with Serializable{\\n    def inputSchema: StructType = new StructType().add(\\\"a\\\", LongType)\\n    def bufferSchema: StructType =  new StructType().add(\\\"total\\\", LongType)\\n    def dataType: DataType = LongType\\n    def deterministic: Boolean = true\\n    def initialize(buffer: MutableAggregationBuffer): Unit = {\\n      buffer.update(0, 0l)\\n    }\\n    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\\n      val sum   = buffer.getLong(0)\\n      val newitem = input.getLong(0)\\n      buffer.update(0, sum + newitem)\\n    }\\n    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\\n      buffer1.update(0, buffer1.getLong(0) + buffer2.getLong(0))\\n    }\\n    def evaluate(buffer: Row): Any = {\\n      buffer.getLong(0)\\n    }\\n}\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n\\n### Some tricks\\n\\nYou can simplify the definition of UDF register like following:\\n\\n```sql\\nregister ScriptUDF.`` as count_board options lang=\\\"python\\\"\\n    and methodName=\\\"apply\\\"\\n    and dataType=\\\"map(string,integer)\\\"\\n    and code='''\\ndef apply(self, s):\\n    from collections import Counter\\n    return dict(Counter(s))\\n    '''\\n;\\n```\\n\\n\\nMulti methods defined onetime is also supported.\\n\\n```sql\\n\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\ndef hello(a:String)={\\n   s\\\"hello: ${a}\\\"\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as plusFun;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as plus, helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\nYou can also define this methods in a class:\\n\\n```sql\\n\\nset plusFun='''\\n\\nclass ScalaScript {\\n    def apply(a:Double,b:Double)={\\n       a + b\\n    }\\n\\n    def hello(a:String)={\\n       s\\\"hello: ${a}\\\"\\n    }\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\nand className=\\\"ScalaScript\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\n\\n\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ScalerInPlace\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"https://en.wikipedia.org/wiki/Feature_scaling\\\">Feature scaling</a>\\n\\n Feature scaling is a method used to normalize the range of independent variables or features of data.\\n In data processing, it is also known as data normalization and\\n is generally performed during the data preprocessing step.\\n\\n Use \\\"load modelParams.`ScalerInPlace` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"SchedulerCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"SchemaCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ScoreCard\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ScriptUDF\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n## Script support\\n\\nScript e.g. Python,Scala nested in MLSQL provides more fine-grained control when doing some ETL tasks, as it allows you\\neasily create SQL function with more powerful language which can do complex logical task.\\n\\nCause the tedious of java's grammar, we will not support java script.\\n\\nBefore use ScriptUDF module, you can use\\n\\n```\\nload modelParams.`ScriptUDF` as output;\\n```\\n\\nto check how to configure this module.\\n\\n### Python UDF Script Example\\n\\n```sql\\n-- using set statement to hold your python script\\n-- Notice that the first parameter of function you defined should be self.\\nset echoFun='''\\n\\ndef apply(self,m):\\n    return m\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be processed more conveniently.\\nload script.`echoFun` as scriptTable;\\n\\n-- register `apply` as UDF named `echoFun`\\nregister ScriptUDF.`scriptTable` as echoFun options\\n-- specify which script you choose\\nand lang=\\\"python\\\"\\n-- As we know python is not strongly typed language, so\\n-- we should manually spcify the return type.\\n-- map(string,string) means a map with key is string type,value also is string type.\\n-- array(string) means a array with string type element.\\n-- nested is support e.g. array(array(map(string,array(string))))\\nand dataType=\\\"map(string,string)\\\"\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect echoFun(map('a','b')) as res from dataTable as output;\\n```\\n\\n### Scala UDF Script Example\\n\\n```sql\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\n''';\\n\\n-- load script as a table, every thing in mlsql should be table which\\n-- can be process more convenient.\\nload script.`plusFun` as scriptTable;\\n\\n-- register `apply` as UDF named `plusFun`\\nregister ScriptUDF.`scriptTable` as plusFun\\n;\\n\\n-- create a data table.\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as res from dataTable as output;\\n```\\n\\n\\n### Python UDAF Example\\n\\n```sql\\nset plusFun='''\\nfrom org.apache.spark.sql.expressions import MutableAggregationBuffer, UserDefinedAggregateFunction\\nfrom org.apache.spark.sql.types import DataTypes,StructType\\nfrom org.apache.spark.sql import Row\\nimport java.lang.Long as l\\nimport java.lang.Integer as i\\n\\nclass SumAggregation:\\n\\n    def inputSchema(self):\\n        return StructType().add(\\\"a\\\", DataTypes.LongType)\\n\\n    def bufferSchema(self):\\n        return StructType().add(\\\"total\\\", DataTypes.LongType)\\n\\n    def dataType(self):\\n        return DataTypes.LongType\\n\\n    def deterministic(self):\\n        return True\\n\\n    def initialize(self,buffer):\\n        return buffer.update(i(0), l(0))\\n\\n    def update(self,buffer, input):\\n        sum = buffer.getLong(i(0))\\n        newitem = input.getLong(i(0))\\n        buffer.update(i(0), l(sum + newitem))\\n\\n    def merge(self,buffer1, buffer2):\\n        buffer1.update(i(0), l(buffer1.getLong(i(0)) + buffer2.getLong(i(0))))\\n\\n    def evaluate(self,buffer):\\n        return buffer.getLong(i(0))\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\nand lang=\\\"python\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n### Scala UDAF Script Example\\n\\n```sql\\nset plusFun='''\\nimport org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\\nimport org.apache.spark.sql.types._\\nimport org.apache.spark.sql.Row\\nclass SumAggregation extends UserDefinedAggregateFunction with Serializable{\\n    def inputSchema: StructType = new StructType().add(\\\"a\\\", LongType)\\n    def bufferSchema: StructType =  new StructType().add(\\\"total\\\", LongType)\\n    def dataType: DataType = LongType\\n    def deterministic: Boolean = true\\n    def initialize(buffer: MutableAggregationBuffer): Unit = {\\n      buffer.update(0, 0l)\\n    }\\n    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\\n      val sum   = buffer.getLong(0)\\n      val newitem = input.getLong(0)\\n      buffer.update(0, sum + newitem)\\n    }\\n    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\\n      buffer1.update(0, buffer1.getLong(0) + buffer2.getLong(0))\\n    }\\n    def evaluate(buffer: Row): Any = {\\n      buffer.getLong(0)\\n    }\\n}\\n''';\\n\\n\\n--加载脚本\\nload script.`plusFun` as scriptTable;\\n--注册为UDF函数 名称为plusFun\\nregister ScriptUDF.`scriptTable` as plusFun options\\nclassName=\\\"SumAggregation\\\"\\nand udfType=\\\"udaf\\\"\\n;\\n\\nset data='''\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n{\\\"a\\\":1}\\n''';\\nload jsonStr.`data` as dataTable;\\n\\n-- 使用plusFun\\nselect a,plusFun(a) as res from dataTable group by a as output;\\n```\\n\\n\\n### Some tricks\\n\\nYou can simplify the definition of UDF register like following:\\n\\n```sql\\nregister ScriptUDF.`` as count_board options lang=\\\"python\\\"\\n    and methodName=\\\"apply\\\"\\n    and dataType=\\\"map(string,integer)\\\"\\n    and code='''\\ndef apply(self, s):\\n    from collections import Counter\\n    return dict(Counter(s))\\n    '''\\n;\\n```\\n\\n\\nMulti methods defined onetime is also supported.\\n\\n```sql\\n\\nset plusFun='''\\n\\ndef apply(a:Double,b:Double)={\\n   a + b\\n}\\n\\ndef hello(a:String)={\\n   s\\\"hello: ${a}\\\"\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as plusFun;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect plusFun(1,2) as plus, helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\nYou can also define this methods in a class:\\n\\n```sql\\n\\nset plusFun='''\\n\\nclass ScalaScript {\\n    def apply(a:Double,b:Double)={\\n       a + b\\n    }\\n\\n    def hello(a:String)={\\n       s\\\"hello: ${a}\\\"\\n    }\\n}\\n\\n''';\\n\\n\\nload script.`plusFun` as scriptTable;\\nregister ScriptUDF.`scriptTable` as helloFun options\\nmethodName=\\\"hello\\\"\\nand className=\\\"ScalaScript\\\"\\n;\\n\\n\\n-- using echoFun in SQL.\\nselect helloFun(\\\"jack\\\") as jack as output;\\n```\\n\\n\\n\\n    \",\n\t\t\t\"docType\": \"md\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"SendMessage\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n SendMessage provides the ability to send messages externally. Currently, it only supports sending messages by mail, and supports single mail and batch mailing. The mail service provides two ways, one is to configure the email account of the mail sender through MLSQL and directly connect to the SMTP service to send mail, and the other is to connect to the local sendmail service to send mail.\\n\\nscenes to be used:\\n1. After the data is calculated and processed, a download link is generated and emailed to relevant personnel\\n2. When the amount of data is small, the data processing result can be sent directly\\n\\n Use \\\"load modelParams.`SendMessage` as output;\\\"\\n to check the available parameters;\\n\\n Use \\\"load modelExample.`SendMessage` as output;\\\"\\n get example.\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"SendMultiMails\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n SendMultiMails provides the ability to send multiple mails externally. The mail service provides two ways, one is to configure the email account of the mail sender through MLSQL and directly connect to the SMTP service to send mail, and the other is to connect to the local sendmail service to send mail.\\n\\nscenes to be used:\\n1. After the data is calculated and processed, send the result to multiple users\\n2. When the amount of data is small, the data processing result can be sent directly\\n\\nconfiguration:\\n1. Set parameters into a table and specify it through the 'paramTab' parameter. Make Sure column names equals to parameter names.\\n2. Set parameters after `where` statement as global config. And parameter with the same name will be overwritten.\\n\\n Use \\\"load modelParams.`SendMultiMails` as output;\\\"\\n to check the available parameters;\\n\\n Use \\\"load modelExample.`SendMultiMails` as output;\\\"\\n get example.\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ShellExecute\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ShowCommand\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ShowFunctionsExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"ShowTableExt\",\n\t\t\t\"algType\": \"undefined\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\",\n\t\t\t\"docType\": \"text\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"!show et;\n!lastCommand named ets;\nselect * from ets where name like \"%Random%\" as output;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"algType\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"sparkCompatibility\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"doc\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"docType\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"name\": \"RandomForest\",\n\t\t\t\"algType\": \"algorithm\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n <a href=\\\"http://en.wikipedia.org/wiki/Random_forest\\\">Random Forest</a> learning algorithm for\\n classification.\\n It supports both binary and multiclass labels, as well as both continuous and categorical\\n features.\\n\\n Use \\\"load modelParams.`RandomForest` as output;\\\"\\n\\n to check the available hyper parameters;\\n\\n Use \\\"load modelExample.`RandomForest` as output;\\\"\\n get example.\\n\\n If you wanna check the params of model you have trained, use this command:\\n\\n ```\\n load modelExplain.`/tmp/model` where alg=\\\"RandomForest\\\" as outout;\\n ```\\n\\n    \",\n\t\t\t\"docType\": \"html\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"TakeRandomSampleExt\",\n\t\t\t\"algType\": \"feature engineer\",\n\t\t\t\"sparkCompatibility\": \"\",\n\t\t\t\"doc\": \"\\n\\n\",\n\t\t\t\"docType\": \"md\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"!show et/RandomForest;\nload modelParams.`RandomForest` as output;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"param\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"description\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"value\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"extra\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"param\": \"evaluateTable\",\n\t\t\t\"description\": \"The table name to evaluate the model performance in training stage\",\n\t\t\t\"value\": \"(undefined)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"evaluateTable\\\",\\\"value\\\":\\\"\\\",\\\"extra\\\":{\\\"doc\\\":\\\"The table name to evaluate the model performance in training stage\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"string\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"undefined\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"keepVersion\",\n\t\t\t\"description\": \"If set true, then every time you run the \\\" +\\n    \\\"algorithm, it will generate a new directory to save the model.\",\n\t\t\t\"value\": \"(default: true)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"keepVersion\\\",\\\"values\\\":[{\\\"name\\\":\\\"keepVersion\\\",\\\"value\\\":\\\"true\\\"},{\\\"name\\\":\\\"keepVersion\\\",\\\"value\\\":\\\"false\\\"}],\\\"extra\\\":{\\\"doc\\\":\\\"If set true, then every time you run the \\\\\\\" +\\\\n    \\\\\\\"algorithm, it will generate a new directory to save the model.\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"boolean\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"true\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Select\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].bootstrap\",\n\t\t\t\"description\": \"Whether bootstrap samples are used when building trees.\",\n\t\t\t\"value\": \"bootstrap: Whether bootstrap samples are used when building trees. (default: true)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"bootstrap\\\",\\\"values\\\":[{\\\"name\\\":\\\"bootstrap\\\",\\\"value\\\":\\\"true\\\"},{\\\"name\\\":\\\"bootstrap\\\",\\\"value\\\":\\\"false\\\"}],\\\"extra\\\":{\\\"doc\\\":\\\"Whether bootstrap samples are used when building trees.\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"boolean\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"true\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Select\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].cacheNodeIds\",\n\t\t\t\"description\": \"If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.\",\n\t\t\t\"value\": \"cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"cacheNodeIds\\\",\\\"values\\\":[{\\\"name\\\":\\\"cacheNodeIds\\\",\\\"value\\\":\\\"true\\\"},{\\\"name\\\":\\\"cacheNodeIds\\\",\\\"value\\\":\\\"false\\\"}],\\\"extra\\\":{\\\"doc\\\":\\\"If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"boolean\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"false\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Select\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].checkpointInterval\",\n\t\t\t\"description\": \"set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext\",\n\t\t\t\"value\": \"checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"checkpointInterval\\\",\\\"value\\\":\\\"Some(10)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"int\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"10\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].featureSubsetStrategy\",\n\t\t\t\"description\": \"The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].\",\n\t\t\t\"value\": \"featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"featureSubsetStrategy\\\",\\\"values\\\":[{\\\"name\\\":\\\"featureSubsetStrategy\\\",\\\"value\\\":\\\"auto\\\"},{\\\"name\\\":\\\"featureSubsetStrategy\\\",\\\"value\\\":\\\"all\\\"},{\\\"name\\\":\\\"featureSubsetStrategy\\\",\\\"value\\\":\\\"onethird\\\"},{\\\"name\\\":\\\"featureSubsetStrategy\\\",\\\"value\\\":\\\"sqrt\\\"},{\\\"name\\\":\\\"featureSubsetStrategy\\\",\\\"value\\\":\\\"log2\\\"},{\\\"name\\\":\\\"featureSubsetStrategy\\\",\\\"value\\\":\\\"(0.0-1.0]\\\"},{\\\"name\\\":\\\"featureSubsetStrategy\\\",\\\"value\\\":\\\"[1-n]\\\"}],\\\"extra\\\":{\\\"doc\\\":\\\"The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"string\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"auto\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Select\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].featuresCol\",\n\t\t\t\"description\": \"features column name\",\n\t\t\t\"value\": \"featuresCol: features column name (default: features)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"featuresCol\\\",\\\"value\\\":\\\"Some(features)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"features column name\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"string\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"features\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].impurity\",\n\t\t\t\"description\": \"Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini\",\n\t\t\t\"value\": \"impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"impurity\\\",\\\"value\\\":\\\"Some(gini)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"string\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"gini\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].labelCol\",\n\t\t\t\"description\": \"label column name\",\n\t\t\t\"value\": \"labelCol: label column name (default: label)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"labelCol\\\",\\\"value\\\":\\\"Some(label)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"label column name\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"string\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"label\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].leafCol\",\n\t\t\t\"description\": \"Leaf indices column name. Predicted leaf index of each instance in each tree by preorder\",\n\t\t\t\"value\": \"leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder (default: )\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"leafCol\\\",\\\"value\\\":\\\"Some()\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Leaf indices column name. Predicted leaf index of each instance in each tree by preorder\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"string\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].maxBins\",\n\t\t\t\"description\": \"Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.\",\n\t\t\t\"value\": \"maxBins: Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature. (default: 32)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"maxBins\\\",\\\"value\\\":\\\"Some(32)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"int\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"32\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].maxDepth\",\n\t\t\t\"description\": \"Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\",\n\t\t\t\"value\": \"maxDepth: Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"maxDepth\\\",\\\"value\\\":\\\"Some(5)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"int\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"5\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].maxMemoryInMB\",\n\t\t\t\"description\": \"Maximum memory in MB allocated to histogram aggregation.\",\n\t\t\t\"value\": \"maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"maxMemoryInMB\\\",\\\"value\\\":\\\"Some(256)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Maximum memory in MB allocated to histogram aggregation.\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"int\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"256\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].minInfoGain\",\n\t\t\t\"description\": \"Minimum information gain for a split to be considered at a tree node.\",\n\t\t\t\"value\": \"minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"minInfoGain\\\",\\\"value\\\":\\\"Some(0.0)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Minimum information gain for a split to be considered at a tree node.\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"double\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"0.0\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].minInstancesPerNode\",\n\t\t\t\"description\": \"Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.\",\n\t\t\t\"value\": \"minInstancesPerNode: Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1. (default: 1)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"minInstancesPerNode\\\",\\\"value\\\":\\\"Some(1)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"int\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"1\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].minWeightFractionPerNode\",\n\t\t\t\"description\": \"Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5)\",\n\t\t\t\"value\": \"minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5) (default: 0.0)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"minWeightFractionPerNode\\\",\\\"value\\\":\\\"Some(0.0)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5)\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"double\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"0.0\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].numTrees\",\n\t\t\t\"description\": \"Number of trees to train (at least 1)\",\n\t\t\t\"value\": \"numTrees: Number of trees to train (at least 1) (default: 20)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"numTrees\\\",\\\"value\\\":\\\"Some(20)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Number of trees to train (at least 1)\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"int\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"20\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].predictionCol\",\n\t\t\t\"description\": \"prediction column name\",\n\t\t\t\"value\": \"predictionCol: prediction column name (default: prediction)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"predictionCol\\\",\\\"value\\\":\\\"Some(prediction)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"prediction column name\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"string\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"prediction\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].probabilityCol\",\n\t\t\t\"description\": \"Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities\",\n\t\t\t\"value\": \"probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"probabilityCol\\\",\\\"value\\\":\\\"Some(probability)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"string\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"probability\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].rawPredictionCol\",\n\t\t\t\"description\": \"raw prediction (a.k.a. confidence) column name\",\n\t\t\t\"value\": \"rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"rawPredictionCol\\\",\\\"value\\\":\\\"Some(rawPrediction)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"raw prediction (a.k.a. confidence) column name\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"string\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"rawPrediction\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].seed\",\n\t\t\t\"description\": \"random seed\",\n\t\t\t\"value\": \"seed: random seed (default: 207336481)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"seed\\\",\\\"value\\\":\\\"Some(207336481)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"random seed\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"long\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"207336481\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].subsamplingRate\",\n\t\t\t\"description\": \"Fraction of the training data used for learning each decision tree, in range (0, 1].\",\n\t\t\t\"value\": \"subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"subsamplingRate\\\",\\\"value\\\":\\\"Some(1.0)\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Fraction of the training data used for learning each decision tree, in range (0, 1].\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"double\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"1.0\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].thresholds\",\n\t\t\t\"description\": \"Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold\",\n\t\t\t\"value\": \"thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"thresholds\\\",\\\"value\\\":\\\"None\\\",\\\"extra\\\":{\\\"doc\\\":\\\"Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"array[double]\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"undefined\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t},\n\t\t{\n\t\t\t\"param\": \"fitParam.[group].weightCol\",\n\t\t\t\"description\": \"weight column name. If this is not set or empty, we treat all instance weights as 1.0\",\n\t\t\t\"value\": \"weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\",\n\t\t\t\"extra\": \"{\\\"name\\\":\\\"weightCol\\\",\\\"value\\\":\\\"None\\\",\\\"extra\\\":{\\\"doc\\\":\\\"weight column name. If this is not set or empty, we treat all instance weights as 1.0\\\",\\\"label\\\":\\\"\\\",\\\"options\\\":{\\\"valueType\\\":\\\"string\\\",\\\"derivedType\\\":\\\"NONE\\\",\\\"required\\\":\\\"false\\\",\\\"defaultValue\\\":\\\"undefined\\\",\\\"currentValue\\\":\\\"undefined\\\"}},\\\"tpe\\\":\\\"Text\\\",\\\"valueProvider\\\":{}}\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"-- use RandomForest\ntrain mock_data_1 as RandomForest.`/tmp/model` where\nkeepVersion=\"true\" \nand evaluateTable=\"mock_data_validate\"\n\nand `fitParam.0.labelCol`=\"label\"\nand `fitParam.0.featuresCol`=\"features\"\nand `fitParam.0.maxDepth`=\"2\"\n\nand `fitParam.1.featuresCol`=\"features\"\nand `fitParam.1.labelCol`=\"label\"\nand `fitParam.1.maxDepth`=\"10\"\n;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"value\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"name\": \"---------------\",\n\t\t\t\"value\": \"------------------\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"modelPath\",\n\t\t\t\"value\": \"/_model_1/model/1\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"algIndex\",\n\t\t\t\"value\": \"1\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"alg\",\n\t\t\t\"value\": \"org.apache.spark.ml.classification.RandomForestClassifier\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"metrics\",\n\t\t\t\"value\": \"f1:  0.7625000000000001\\nweightedPrecision:  0.8444444444444446\\nweightedRecall:  0.7999999999999999\\naccuracy:  0.8\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"status\",\n\t\t\t\"value\": \"success\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"message\",\n\t\t\t\"value\": \"\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"startTime\",\n\t\t\t\"value\": \"20220904 54:22:55:242\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"endTime\",\n\t\t\t\"value\": \"20220904 54:22:56:104\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"trainParams\",\n\t\t\t\"value\": \"Map(labelCol -> label, featuresCol -> features, maxDepth -> 10)\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"---------------\",\n\t\t\t\"value\": \"------------------\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"modelPath\",\n\t\t\t\"value\": \"/_model_1/model/0\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"algIndex\",\n\t\t\t\"value\": \"0\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"alg\",\n\t\t\t\"value\": \"org.apache.spark.ml.classification.RandomForestClassifier\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"metrics\",\n\t\t\t\"value\": \"f1:  0.7625000000000001\\nweightedPrecision:  0.8444444444444446\\nweightedRecall:  0.7999999999999999\\naccuracy:  0.8\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"status\",\n\t\t\t\"value\": \"success\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"message\",\n\t\t\t\"value\": \"\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"startTime\",\n\t\t\t\"value\": \"20220904 54:22:56:105\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"endTime\",\n\t\t\t\"value\": \"20220904 54:22:56:869\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"trainParams\",\n\t\t\t\"value\": \"Map(maxDepth -> 2, featuresCol -> features, labelCol -> label)\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"predict mock_data_test as RandomForest.`/tmp/model`  as predicted_table;\n-- to display features \n-- select vec_array(features) as features from predicted_table as ouput;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"features\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"udt\",\n\t\t\t\t\t\"class\": \"org.apache.spark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"pyClass\": \"pyspark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"sqlType\": {\n\t\t\t\t\t\t\"type\": \"struct\",\n\t\t\t\t\t\t\"fields\": [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"type\",\n\t\t\t\t\t\t\t\t\"type\": \"byte\",\n\t\t\t\t\t\t\t\t\"nullable\": false,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"size\",\n\t\t\t\t\t\t\t\t\"type\": \"integer\",\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"indices\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"integer\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"values\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"label\",\n\t\t\t\t\"type\": \"double\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"rawPrediction\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"udt\",\n\t\t\t\t\t\"class\": \"org.apache.spark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"pyClass\": \"pyspark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"sqlType\": {\n\t\t\t\t\t\t\"type\": \"struct\",\n\t\t\t\t\t\t\"fields\": [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"type\",\n\t\t\t\t\t\t\t\t\"type\": \"byte\",\n\t\t\t\t\t\t\t\t\"nullable\": false,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"size\",\n\t\t\t\t\t\t\t\t\"type\": \"integer\",\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"indices\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"integer\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"values\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {\n\t\t\t\t\t\"ml_attr\": {\n\t\t\t\t\t\t\"num_attrs\": 2\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"probability\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"udt\",\n\t\t\t\t\t\"class\": \"org.apache.spark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"pyClass\": \"pyspark.ml.linalg.VectorUDT\",\n\t\t\t\t\t\"sqlType\": {\n\t\t\t\t\t\t\"type\": \"struct\",\n\t\t\t\t\t\t\"fields\": [\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"type\",\n\t\t\t\t\t\t\t\t\"type\": \"byte\",\n\t\t\t\t\t\t\t\t\"nullable\": false,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"size\",\n\t\t\t\t\t\t\t\t\"type\": \"integer\",\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"indices\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"integer\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\"name\": \"values\",\n\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\t\t\t\t\"containsNull\": false\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"nullable\": true,\n\t\t\t\t\t\t\t\t\"metadata\": {}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t]\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {\n\t\t\t\t\t\"ml_attr\": {\n\t\t\t\t\t\t\"num_attrs\": 2\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"prediction\",\n\t\t\t\t\"type\": \"double\",\n\t\t\t\t\"nullable\": false,\n\t\t\t\t\"metadata\": {\n\t\t\t\t\t\"ml_attr\": {\n\t\t\t\t\t\t\"type\": \"nominal\",\n\t\t\t\t\t\t\"num_vals\": 2\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.635911310911313,\n\t\t\t\t\t5.364088689088689\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7317955655455657,\n\t\t\t\t\t0.2682044344544344\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.635911310911313,\n\t\t\t\t\t5.364088689088689\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7317955655455657,\n\t\t\t\t\t0.2682044344544344\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.635911310911313,\n\t\t\t\t\t5.364088689088689\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7317955655455657,\n\t\t\t\t\t0.2682044344544344\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t4.4,\n\t\t\t\t\t2.9,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.75257797757798,\n\t\t\t\t\t5.247422022422023\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7376288988788988,\n\t\t\t\t\t0.26237110112110107\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.635911310911313,\n\t\t\t\t\t5.364088689088689\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7317955655455657,\n\t\t\t\t\t0.2682044344544344\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.635911310911313,\n\t\t\t\t\t5.364088689088689\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7317955655455657,\n\t\t\t\t\t0.2682044344544344\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.635911310911313,\n\t\t\t\t\t5.364088689088689\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7317955655455657,\n\t\t\t\t\t0.2682044344544344\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t4.7,\n\t\t\t\t\t3.2,\n\t\t\t\t\t1.3,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 1,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.265384615384615,\n\t\t\t\t\t14.734615384615383\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.26326923076923076,\n\t\t\t\t\t0.7367307692307692\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 1\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.635911310911313,\n\t\t\t\t\t5.364088689088689\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7317955655455657,\n\t\t\t\t\t0.2682044344544344\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t},\n\t\t{\n\t\t\t\"features\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t5.1,\n\t\t\t\t\t3.5,\n\t\t\t\t\t1.4,\n\t\t\t\t\t0.2\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"label\": 0,\n\t\t\t\"rawPrediction\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t14.635911310911313,\n\t\t\t\t\t5.364088689088689\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"probability\": {\n\t\t\t\t\"type\": 1,\n\t\t\t\t\"values\": [\n\t\t\t\t\t0.7317955655455657,\n\t\t\t\t\t0.2682044344544344\n\t\t\t\t]\n\t\t\t},\n\t\t\t\"prediction\": 0\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"run predicted_table as ConfusionMatrix.`/tmp/models/model_acc` where \nactualCol=\"label\" and \npredictCol=\"prediction\";\nload parquet.`/tmp/models/model_acc/detail` as output;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"lable\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"value\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"desc\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"PPV\",\n\t\t\t\"value\": \"0.7777777777777778\",\n\t\t\t\"desc\": \"Precision or positive prediction value\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"FN\",\n\t\t\t\"value\": \"0\",\n\t\t\t\"desc\": \"False negative [eqv with miss, Type II error]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"FN\",\n\t\t\t\"value\": \"2\",\n\t\t\t\"desc\": \"False negative [eqv with miss, Type II error]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"PPV\",\n\t\t\t\"value\": \"1.0\",\n\t\t\t\"desc\": \"Precision or positive prediction value\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"FDR\",\n\t\t\t\"value\": \"0.2222222222222222\",\n\t\t\t\"desc\": \"False discovery rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"TPR\",\n\t\t\t\"value\": \"1.0\",\n\t\t\t\"desc\": \"Sensitivity or true positive rate [eqv with hit rate, recall]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"SPC\",\n\t\t\t\"value\": \"0.3333333333333333\",\n\t\t\t\"desc\": \"Specificity or true negative rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"TPR\",\n\t\t\t\"value\": \"0.3333333333333333\",\n\t\t\t\"desc\": \"Sensitivity or true positive rate [eqv with hit rate, recall]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"SPC\",\n\t\t\t\"value\": \"1.0\",\n\t\t\t\"desc\": \"Specificity or true negative rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"TN\",\n\t\t\t\"value\": \"7\",\n\t\t\t\"desc\": \"True negative [eqv with correct rejection]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"FP\",\n\t\t\t\"value\": \"0\",\n\t\t\t\"desc\": \"False positive [eqv with false alarm, Type I error]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"TN\",\n\t\t\t\"value\": \"1\",\n\t\t\t\"desc\": \"True negative [eqv with correct rejection]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"FP\",\n\t\t\t\"value\": \"2\",\n\t\t\t\"desc\": \"False positive [eqv with false alarm, Type I error]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"TP\",\n\t\t\t\"value\": \"1\",\n\t\t\t\"desc\": \"True positive [eqv with hit]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"TP\",\n\t\t\t\"value\": \"7\",\n\t\t\t\"desc\": \"True positive [eqv with hit]\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"NPV\",\n\t\t\t\"value\": \"1.0\",\n\t\t\t\"desc\": \"Negative predictive value\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"FPR\",\n\t\t\t\"value\": \"0.6666666666666666\",\n\t\t\t\"desc\": \"Fall-out or false positive rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"NPV\",\n\t\t\t\"value\": \"0.7777777777777778\",\n\t\t\t\"desc\": \"Negative predictive value\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"FPR\",\n\t\t\t\"value\": \"0.0\",\n\t\t\t\"desc\": \"Fall-out or false positive rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"FDR\",\n\t\t\t\"value\": \"0.0\",\n\t\t\t\"desc\": \"False discovery rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"FNR\",\n\t\t\t\"value\": \"0.6666666666666666\",\n\t\t\t\"desc\": \"Miss Rate or False Negative Rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"1.0\",\n\t\t\t\"name\": \"ACC\",\n\t\t\t\"value\": \"0.8\",\n\t\t\t\"desc\": \"Accuracy\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"FNR\",\n\t\t\t\"value\": \"0.0\",\n\t\t\t\"desc\": \"Miss Rate or False Negative Rate\"\n\t\t},\n\t\t{\n\t\t\t\"lable\": \"0.0\",\n\t\t\t\"name\": \"ACC\",\n\t\t\t\"value\": \"0.8\",\n\t\t\t\"desc\": \"Accuracy\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"register RandomForest.`/tmp/model` as model_predict;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"value\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"name\": \"uid\",\n\t\t\t\"value\": \"rfc_9bdf6b1e5be6\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"numFeatures\",\n\t\t\t\"value\": \"4\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"numClasses\",\n\t\t\t\"value\": \"2\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"numTrees\",\n\t\t\t\"value\": \"20\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"treeWeights\",\n\t\t\t\"value\": \"1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"fitParam.[group].featuresCol\",\n\t\t\t\"value\": \"features\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"fitParam.[group].labelCol\",\n\t\t\t\"value\": \"label\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"fitParam.[group].maxDepth\",\n\t\t\t\"value\": \"10\"\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"select vec_array(model_predict(features)) from mock_data_test as output;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"vec_array(UDF(features))\",\n\t\t\t\t\"type\": {\n\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\"elementType\": \"double\",\n\t\t\t\t\t\"containsNull\": false\n\t\t\t\t},\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"vec_array(UDF(features))\": [\n\t\t\t\t0.7317955655455657,\n\t\t\t\t0.2682044344544344\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"vec_array(UDF(features))\": [\n\t\t\t\t0.7317955655455657,\n\t\t\t\t0.2682044344544344\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"vec_array(UDF(features))\": [\n\t\t\t\t0.7317955655455657,\n\t\t\t\t0.2682044344544344\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"vec_array(UDF(features))\": [\n\t\t\t\t0.7376288988788988,\n\t\t\t\t0.26237110112110107\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"vec_array(UDF(features))\": [\n\t\t\t\t0.7317955655455657,\n\t\t\t\t0.2682044344544344\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"vec_array(UDF(features))\": [\n\t\t\t\t0.7317955655455657,\n\t\t\t\t0.2682044344544344\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"vec_array(UDF(features))\": [\n\t\t\t\t0.7317955655455657,\n\t\t\t\t0.2682044344544344\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"vec_array(UDF(features))\": [\n\t\t\t\t0.26326923076923076,\n\t\t\t\t0.7367307692307692\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"vec_array(UDF(features))\": [\n\t\t\t\t0.7317955655455657,\n\t\t\t\t0.2682044344544344\n\t\t\t]\n\t\t},\n\t\t{\n\t\t\t\"vec_array(UDF(features))\": [\n\t\t\t\t0.7317955655455657,\n\t\t\t\t0.2682044344544344\n\t\t\t]\n\t\t}\n\t]\n}"}]},{"kind":2,"language":"mlsql","value":"load modelExplain.`/tmp/model` \nwhere alg=\"RandomForest\"\nas output;","outputs":[{"mime":"x-application/mlsql-notebook","value":"{\n\t\"schema\": {\n\t\t\"type\": \"struct\",\n\t\t\"fields\": [\n\t\t\t{\n\t\t\t\t\"name\": \"name\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t},\n\t\t\t{\n\t\t\t\t\"name\": \"value\",\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"nullable\": true,\n\t\t\t\t\"metadata\": {}\n\t\t\t}\n\t\t]\n\t},\n\t\"data\": [\n\t\t{\n\t\t\t\"name\": \"uid\",\n\t\t\t\"value\": \"rfc_9bdf6b1e5be6\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"numFeatures\",\n\t\t\t\"value\": \"4\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"numClasses\",\n\t\t\t\"value\": \"2\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"numTrees\",\n\t\t\t\"value\": \"20\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"treeWeights\",\n\t\t\t\"value\": \"1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"fitParam.[group].featuresCol\",\n\t\t\t\"value\": \"features\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"fitParam.[group].labelCol\",\n\t\t\t\"value\": \"label\"\n\t\t},\n\t\t{\n\t\t\t\"name\": \"fitParam.[group].maxDepth\",\n\t\t\t\"value\": \"10\"\n\t\t}\n\t]\n}"}]}]